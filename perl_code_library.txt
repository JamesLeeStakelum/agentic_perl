TASK ASSIGNMENT:
study the information (below).
the info is rather disorganized. just copy pasted from my notes. i need you to organize it better.

the intended use: your reorganized version of this info will be included in my LLM calls that are coding Perl scripts. it will tell the LLM about things like:
- my preferred design patterns
- functions i use in my perl library

it should make it clear that the LLM must always use my functions, instead of making its own. and that is must use my design pattenrs.

####################################


I use primarily Perl coding for my LLM projects.
And, there is a standard set of design pattnerns i use.
and a standard set of Perl functions I use from my function library.
some of the main functions i use include:

read_file() - logic to read any text file, determine the encoding, and works with pretty much any encoding you should ever expect to see (ASCII, UTF8, utf8 bom, ...).

write_file() - writes output file as utf8 encoding

call_llm() - this calls a compiled exe command line utility that makes the LLM calls. i pass params, one for input file, one for output file. it connects to the LLM, sends the text of the input file as the prompt, and writes the response to the output file.

extract_text_between_tags() - in my prompts, i ALWAYS tell the LLM to writte its answer inside <answer>...</answer> tags, and write any comments inside <comments>..</comments> tags. the extract_text_between_tags func extracts the text inside the tags. I pass it the text from the llm response, and the name of the tag containing the section from the response I want to extract, and i get back that text.

remove_non_ascii() - i always tell the LLM in my prompt to avoid curly quotes, emoticons, curly apostrophe, etc, but LLM often ignores that. so, i can call remove_non_ascii() to remove the emoticons, and transform curly quote and curly apostrophes to straight quotes and straight apostrophes.

hill_climbing() - for most ordinary LLM requests, i use call_llm(). but if i need reflection, in which LLM response is sent back to the LLM along with request to try to improve upon the candidate response, i use hill_climbing() to do that.

ensure_directory() -  Ensures that the given directory exists, creating any intermediate directories as needed.


##############################################
FUNCTIONS TO USE FOR MAKING LLM CALLS
##############################################

LLM Integration Workflow for an Application

Purpose and Context:
The application must communicate with an external large language model (LLM) using a command-line tool (e.g., call_openai.com, call_openrouter.exe). This tool writes a given prompt to a temporary input file, makes a REST API call to the LLM using configuration data (such as an API key stored in a separate configuration file), and writes the LLM s response to an output file. The application then reads the response for further processing.

Primary Functions for LLM Calls:
Two core functions are provided:

1. call_llm
   This function sends a prompt to the external LLM tool. It performs the following steps:
   - Generates unique temporary file names using a random string.
   - Writes the provided prompt (in UTF-8) to an input file in the .\temp folder.
   - Constructs and executes a system command to invoke call_openrouter.exe, which uses pre-stored configuration (such as API keys) to call the external LLM service.
   - Reads the response from the output file and trims any extra whitespace.
   Use call_llm when you have a clear and well-formed prompt and expect a complete response.






Structured Output Requirement:
It is highly recommended that prompts be prepared to instruct the LLM to produce structured output. In your prompt, specify that the LLM should enclose its answer within specific tag pairs (for example, <answer> and </answer>). Additionally, instruct the LLM to wrap any extra commentary or meta-information in another set of tags (for example, <comments> and </comments>). This structure ensures that the application can later extract only the desired content.

Using extract_text_between_tags:
The extract_text_between_tags function is a very convenient tool for parsing the LLM s response. After receiving the full response (from call_llm or call_llm_with_retries), your application should call extract_text_between_tags for each tag pair that you expect in the response. For example, to extract the main answer, you would use:
   my $final_answer = extract_text_between_tags($response, "answer");
If additional information is provided in a separate tag, you can similarly extract it:
   my $comments = extract_text_between_tags($response, "comments");
This method reliably separates the intended answer from any extraneous content, ensuring that the application processes only the relevant information.

Workflow Steps for an Application:
Step 1: Prepare the Prompt
   - Construct a clear and well-formatted prompt instructing the LLM to output its answer within <answer> tags and any commentary within <comments> tags.
   - For example, include instructions such as: "Please generate your answer and enclose it within <answer> and </answer> tags. If you include additional commentary, enclose it within <comments> and </comments> tags."

Step 2: Call the LLM
   - Use call_llm_with_retries (or call_llm directly, if appropriate) to send the prompt and receive the response. This helps handle any transient issues with incomplete responses.

Step 3: Process the Response
   - Once the LLM returns its response, trim any extra whitespace.
   - Use extract_text_between_tags to extract the text enclosed in <answer> tags.
   - Optionally, use the same function to extract any text in <comments> tags.
   - Process the extracted answer further according to your application s needs.




Best Practices:
   - Ensure that prompts are structured with clear tag instructions so that the LLM output is predictable and easy to parse.
   - Use extract_text_between_tags to reliably separate the answer from any extraneous commentary.
   - Manage temporary input and output files carefully to support debugging and analysis.
   - Keep sensitive configuration details (such as API keys) in separate configuration files rather than hardcoding them.
   - Integrate these functions into higher-level workflows that handle additional tasks like file I/O, research, or formatting.
   - Thoroughly test these functions under various conditions to ensure that the retry logic and structured output extraction work as intended.

This integrated approach provides a robust and maintainable method for an application to communicate with external LLM services while ensuring that responses are structured, easy to parse, and reliable.








#####################

Pattern Name: Response Tagging
Category: Core Structural Patterns
Keywords: Response Parsing, Data Extraction, Output Formatting
Related Patterns: None
Summary: Encapsulates the LLM's response within specific tags for easy parsing and extraction of the desired information.
Intent: To improve the structure and usability of LLM responses by separating the answer from other parts of the output.
Problem: LLMs often produce responses that include extraneous information, comments, or explanations alongside the actual answer, making it difficult to extract the desired output.
Why Needed:  Clear separation of answer components allows for efficient post-processing and integration with downstream systems.
Prerequisites:  The LLM must be capable of generating responses that include the specified tags.
Overview: The Response Tagging pattern instructs the LLM to wrap its response within predefined tags.  This allows subsequent code to easily identify and extract the answer without needing to parse the entire response.
Steps:
1.  Provide a prompt that explicitly requests the LLM to format its response using specific tags.  For example: "Show your answer, wrapping it inside tags <answer> and </answer>."
2.  The LLM generates a response containing the answer enclosed within the specified tags.
3.  Post-processing code extracts the content between the tags to obtain the answer.

Code Snippet (Optional):
```python
def get_answer(prompt):
    response = llm(prompt)
    start_tag = "<answer>"
    end_tag = "</answer>"
    start_index = response.find(start_tag)
    if start_index == -1:
        return "No answer found"
    end_index = response.find(end_tag, start_index + len(start_tag))
    if end_index == -1:
        return "Incomplete answer"
    answer = response[start_index + len(start_tag):end_index]
    return answer
```

Use Cases:
- Extracting specific information from a longer LLM response.
- Parsing answers from multiple-choice questions.
- Isolating key facts from summaries or reports.

When to Use:
- When the LLM's response contains extraneous information that needs to be filtered.
- When subsequent processing requires a structured output format.

When NOT to Use:
- When the LLM's response is already structured in a way that doesn't require tagging.
- When the answer is very short and the tags would significantly increase the response size.

Best Practices:
- Use consistent and unambiguous tags for clarity and maintainability.
- Ensure the tags are not part of the answer itself to avoid errors.

Challenges:
- The LLM might not always follow the tagging instructions.
- Incorrect tag usage can lead to errors in extraction.

Participants:
- LLM: Generates the response with tags.
- Post-processing code: Extracts the answer from the tagged response.

Pros:
- Improved data extraction efficiency.
- Reduced complexity in parsing the LLM's output.
- Enhanced reliability of extracting the desired information.

Cons:
- Requires additional code for parsing the tagged response.
- Potential for errors if the LLM doesn't follow the tagging instructions.

Pitfalls:
- Incorrectly formatted tags in the prompt.
- Missing or malformed tags in the LLM's response.

Anti-Patterns:
- Relying on the LLM to extract information without providing clear instructions.
- Using ambiguous or inconsistent tags.

Related Patterns:
- None

Implementation:
1. Define the tags to be used for the answer.
2. Include the tag instructions in the prompt.
3. Write code to parse the response and extract the answer.

Variants:
- Nested tags for hierarchical data structures.
- Different tag formats for different types of answers.

Metrics (Optional):
- Time taken to extract the answer from the tagged response.
- Accuracy of the extraction process.

Visual Diagram (Optional):
[A simple diagram showing the prompt, LLM response with tags, and the extraction process would be helpful here, but cannot be rendered in ASCII.]

########################################################################

Pattern Name: Comments Section
Category: Core Structural Patterns
Keywords: Output Structure, Commenting, Parsing, Response Formatting
Related Patterns: None
Summary: Enforces a dedicated section for comments within LLM responses.
Intent: To improve the structure and parsability of LLM outputs by separating comments from the main answer.
Problem: LLMs often include comments or explanations within the main response, making it difficult to extract the core answer for subsequent processing.
Why Needed: Separating comments from the answer stream allows for easier parsing, extraction of key information, and integration with downstream systems.
Prerequisites: None
Overview: This pattern instructs the LLM to format its response with a dedicated section for comments, enclosed within specific tags. This structure simplifies the extraction of the answer from the response.
Steps:
1.  Instruct the LLM to include a comments section.
2.  Define specific tags (e.g., <comments>, </comments>) to enclose the comments.
3.  Parse the response to extract the answer from the main content and the comments from the designated section.
Code Snippet (Optional):
def solve_problem(problem):
    prompt = "Answer the question: " + problem + "  If you have any comments, wrap them inside tags <comments> and </comments>."
    response = llm(prompt)
    try:
        answer = response.split("<comments>")[0].strip()
        comments = response.split("<comments>", 1)[1].split("</comments>", 1)[0].strip()
        return answer, comments
    except IndexError:
        return response, ""  # Handle cases where <comments> tags are missing

Use Cases:
- Extracting factual information from a complex response.
- Parsing summaries or explanations from a lengthy output.
- Integrating LLM responses into other systems that require structured data.
When to Use:
- When the LLM response contains both an answer and comments or explanations.
- When subsequent processing requires separating the answer from the comments.
When NOT to Use:
- When the LLM response is concise and doesn't require separation of comments.
- When the comments are not relevant to the subsequent processing.
Best Practices:
- Use consistent and unambiguous tags for comments.
- Clearly define the expected format in the prompt.
- Handle potential errors (e.g., missing tags) gracefully.
Challenges:
- Ensuring the LLM understands and adheres to the specified format.
- Adapting to different LLM outputs and response styles.
Participants:
- LLM: Generates the response with a dedicated comments section.
- Parser/Processor: Extracts the answer and comments from the structured output.
Pros:
- Improved response structure for easier parsing.
- Enhanced data extraction and integration with downstream systems.
- Reduced ambiguity in the output.
Cons:
- Potential for increased response length if the LLM generates verbose comments.
- Requires additional parsing logic to extract the answer and comments.
Pitfalls:
- Incorrectly formatted tags in the LLM response.
- Missing or inconsistent tag usage.
Anti-Patterns:
- Relying on the LLM to include comments without explicit instructions.
- Using ambiguous or inconsistent tag formats.
Related Patterns:
- None
Implementation:
1.  Craft a prompt that instructs the LLM to use the specified tags for comments.
2.  Implement a parser to extract the answer and comments from the response.
3.  Handle cases where the tags are missing or malformed.
Variants:
- Using different tag formats (e.g., [comments], {comments}).
- Allowing for multiple comment sections.
Metrics (Optional):
- Parsing accuracy (percentage of responses successfully parsed).
- Time taken for parsing.
Visual Diagram (Optional):
[A diagram showing the LLM generating a response with a <comments> section, and a parser extracting the answer and comments.]

########################################################################

Pattern Name: Multiple Response Sections
Category: Core Structural Patterns
Keywords: Data Extraction, Structured Output, Response Formatting
Related Patterns: None
Summary: A design pattern for eliciting multiple responses from an LLM by using distinct tags to delineate different output sections.
Intent: To improve the structure and organization of LLM responses, enabling easier extraction and processing of different parts of the output.
Problem: LLMs often produce unstructured text responses that are difficult to parse and extract specific information.  This can lead to errors in subsequent processing steps.
Why Needed:  Structured responses facilitate efficient data extraction, enabling more robust and reliable downstream applications.
Prerequisites:  The LLM must be capable of understanding and responding to the specified tags.
Overview: This pattern leverages distinct tags to encapsulate different pieces of information within the LLM's response.  This allows for clear separation of output components, making it easier to extract and process specific data elements.

Steps:
1. Define the desired output sections (e.g., title, summary, outline).
2. Use specific tags to enclose each section (e.g., <title>, </title>, <summary>, </summary>).
3. Provide clear instructions to the LLM to format its response using the defined tags.
4. Parse the LLM's response to extract the data within each tag.

Code Snippet (Optional):
```
def get_multiple_responses(prompt):
    response = llm(prompt)
    title = extract_text_from_tags(response, "<title>", "</title>")
    summary = extract_text_from_tags(response, "<summary>", "</summary>")
    # ... extract other sections similarly
    return {"title": title, "summary": summary, ...}

def extract_text_from_tags(text, start_tag, end_tag):
    start_index = text.find(start_tag)
    if start_index == -1:
        return ""
    end_index = text.find(end_tag, start_index + len(start_tag))
    if end_index == -1:
        return ""
    return text[start_index + len(start_tag):end_index]
```

Use Cases:
- Generating structured reports with titles, summaries, and details.
- Extracting specific information from complex documents.
- Creating structured data for further analysis or processing.

When to Use:
- When multiple pieces of information are required from the LLM.
- When subsequent processing needs to access specific parts of the response.

When NOT to Use:
- For simple tasks where a single response is sufficient.
- When the LLM is not capable of understanding and responding to the specified tags.

Best Practices:
- Use consistent and unambiguous tags for each section.
- Clearly define the expected format in the prompt.
- Validate the extracted data to ensure accuracy.

Challenges:
- Potential for errors if the LLM does not follow the specified format.
- Complexity in parsing the response if the format is not well-defined.

Participants:
- LLM: Generates the response with the specified tags.
- Application: Parses the response and extracts the data.

Pros:
- Improved data extraction and processing.
- Enhanced response structure and organization.
- Reduced ambiguity in the output.

Cons:
- Increased complexity in the prompt and response handling.
- Potential for errors if the LLM does not follow the format.

Pitfalls:
- Incorrectly formatted tags in the response.
- Missing tags or malformed tags.

Anti-Patterns:
- Attempting to extract data from unstructured responses.
- Using generic tags that are not specific enough.

Related Patterns:
- None

Implementation:
1. Define the tags for each section.
2. Construct a prompt that includes the tags.
3. Parse the response to extract the data from each tag.

Variants:
- Nested tags for hierarchical data structures.
- Custom tag formats for specific needs.

Metrics (Optional):
- Time taken to extract data from the response.
- Accuracy of data extraction.

Visual Diagram (Optional):
[Diagram illustrating the pattern, showing the LLM generating a response with tags, and the application parsing the response]

########################################################################

Pattern Name: Extracting Tagged Content
Category: Core Structural Patterns
Keywords: Post-processing, Output Formatting, Data Extraction
Related Patterns: None
Summary: Extracts specific content from an LLM response based on predefined tags.
Intent: To isolate and retrieve information from an LLM response that is structured with tags.
Problem: LLMs often return responses with structured information, but accessing specific parts of the response requires manual parsing.
Why Needed:  Automated extraction of tagged content simplifies subsequent processing and allows for more efficient use of LLM outputs.
Prerequisites:
- The LLM response must contain the desired tags.
- Programming language support for parsing the response.
- Knowledge of the tags used in the LLM response.

Overview: This pattern involves parsing the LLM response to locate and extract text enclosed within specific tags.  The extracted text is then assigned to variables for further processing.

Steps:
1. Receive the LLM response.
2. Identify the tags containing the desired content (e.g., 'thinking', 'answer', 'comments').
3. Use a programming language (e.g., Python) to parse the response string.
4. Extract the text enclosed within the identified tags.
5. Assign the extracted text to variables for later use.

Code Snippet (Optional):
```python
import re

def extract_tagged_content(response, tag):
    match = re.search(f"<{tag}>(.*?)</{tag}>", response, re.DOTALL)
    if match:
        return match.group(1)
    else:
        return None

# Example usage
response = "This is the LLM's thinking<thinking>This is the thought process.</thinking>This is the answer<answer>The answer is here.</answer>"
thinking_text = extract_tagged_content(response, "thinking")
answer_text = extract_tagged_content(response, "answer")

print(f"Thinking: {thinking_text}")
print(f"Answer: {answer_text}")
```

Use Cases:
- Extracting the reasoning behind an LLM's answer.
- Isolating specific parts of a generated text (e.g., summary, details).
- Retrieving comments or feedback from the LLM.

When to Use:
- When the LLM response is structured with tags.
- When specific parts of the response need to be isolated for further processing.

When NOT to Use:
- When the LLM response is not tagged.
- When the desired content is not easily identifiable by tags.

Best Practices:
- Use consistent and well-defined tags in the LLM prompts.
- Validate the extracted content to ensure accuracy.
- Handle cases where the tag is not found in the response.

Challenges:
- Potential for errors if the tag structure is inconsistent.
- Performance overhead if the response is very large.

Participants:
- LLM: Generates the response with tags.
- Parsing/Extraction Script: Extracts the tagged content.
- Subsequent Processing: Uses the extracted content.

Pros:
- Automated extraction of specific information.
- Improved efficiency in processing LLM responses.
- Reduced manual effort.

Cons:
- Requires parsing code to extract the content.
- Potential for errors if the tag structure is not consistent.

Pitfalls:
- Incorrect tag names or patterns.
- Missing or malformed tags in the response.

Anti-Patterns:
- Trying to extract content without using tags.
- Using complex regex patterns when simpler ones suffice.

Related Patterns:
- None

Implementation:
1. Define the tags to extract.
2. Write a function to parse the response and extract the content.
3. Call the function to extract the content.
4. Assign the extracted content to variables.

Variants:
- Using different parsing libraries (e.g., BeautifulSoup for HTML-like structures).
- Handling nested tags.

Metrics (Optional):
- Time taken to extract the content.
- Accuracy of the extraction.

Visual Diagram (Optional):
[Diagram would be helpful here, but not possible in ASCII]

########################################################################


###### Chapter One: The Five Pillars of Agentic Programming

**Introduction**

Getting truly great results from large language models (LLMs) isn’t about that first flash of output—it’s about the journey of refinement. **Let me tell you,** over the years, I’ve discovered that building intelligent, self-improving systems (what I call "agentic programming") rests on five foundational pillars. Think of these like the essential legs of a sturdy table. If one is missing or weak, the whole thing becomes unstable. In this chapter, we’ll explore each pillar with practical examples, pseudocode snippets, and insights into how they shape successful agentic systems.

**Pillar 1: Iterative Improvement (Hill Climbing)**

*Concept Overview*
Imagine trying to reach the top of a hill in dense fog. You take a step, check if it feels like you're going uphill, and if so, you continue. That’s the essence of the hill climbing algorithm. In agentic programming, we start with an initial output from an LLM and then iteratively ask it to improve upon its previous attempt. The goal isn't to settle for the first version but to continuously refine until you achieve a high-quality result.

*Simple Example*
A basic loop might look like this:

```
best_version = generate_initial_output(prompt)
for i in range(max_iterations):
    candidate = generate_candidate_revision(best_version)
    if candidate is better than best_version:
        best_version = candidate
    else:
        break
return best_version
```

Here, “better” might initially mean something simple, like the candidate includes a specific keyword or is slightly more detailed. But as we'll see, we quickly need more precise ways to define and measure improvement.

*Refining with Evaluation Criteria*
To ensure objective improvement, you can refine the process by providing explicit evaluation criteria. For instance, when translating a text, “better” means that the translation is clearer, more accurate, and preserves the original tone. **I learned early on that vague notions of "better" weren't enough.** The loop then evolves:

```
best_version = generate_initial_output(prompt)
criteria = "Accurate, clear, and natural translation"
for i in range(max_iterations):
    candidate = generate_candidate_revision(best_version)
    evaluation = evaluate_versions(best_version, candidate, criteria)
    if evaluation indicates candidate is better:
        best_version = candidate
    else:
        break
return best_version
```

*Advanced Hill Climbing: The Panel of Judges*
Even with explicit criteria, a single evaluation can be subjective. That’s where a panel of judges comes in. Imagine using three or five LLM instances (or varied configurations of the same model) to evaluate candidate revisions. Each “judge” reviews the current best version and the candidate, scoring them based on the criteria. Only if a majority agrees does the candidate become the new best. **This approach, I found, significantly reduced the risk of getting stuck on a locally good but not globally optimal solution.**

```
best_version = generate_initial_output(prompt)
criteria = "Clarity, completeness, and appropriate tone"
panel_of_judges = create_llm_instances(num_judges=3)
for i in range(max_iterations):
    candidate = generate_candidate_revision(best_version)
    votes = []
    for judge in panel_of_judges:
        vote = judge_evaluate(best_version, candidate, criteria)
        votes.append(vote)
    if majority(votes) indicates candidate is better:
        best_version = candidate
    else:
        break
return best_version
```

This method minimizes bias and raises the overall quality of the output.

**Pillar 2: Atomic Task Breakdown (Granular Task Planning)**

*The Need for Granularity*
Complex tasks can feel like trying to eat an entire elephant in one bite – impossible! Instead, breaking them into smaller, atomic tasks makes them far more manageable. **Think of it like building with LEGOs** – small, individual bricks can be combined to create incredibly complex and detailed structures. This is the essence of atomic task breakdown.

*Task Planning in Action*
Consider writing a book. Rather than asking an LLM to generate the entire manuscript, you can:
- **Outline the book:** Define chapters and core themes.
- **Break down each chapter:** Identify key topics or sections.
- **Decompose further:** Focus on specific arguments or narrative details.
- **Iteratively refine:** Generate and improve each small unit before assembling the whole. **This granular approach, I've found, makes the entire process much less daunting.**

In software development, the same principle applies: design the architecture, list data structures and functions, and then build and test each function individually.

*The Iterative Breakdown Process*
This approach follows these steps:
- Start with a clear description of the high-level task.
- Decompose it into sub-tasks.
- Continue breaking down until each task is as atomic as needed.
- Solve each atomic task individually, using iterative improvement.
- Reassemble the solutions to form the final output.

This not only simplifies the LLM’s workload but also makes errors easier to spot and fix early on.

**Pillar 3: The Second Mind (Results and Consistency Checking)**

*Avoiding “Amnesia” in Iterative Processes*
As we refine our outputs, sometimes valuable information can be lost—what I call “amnesia.” **One frustrating issue I kept encountering was losing good ideas during refinement, which led to the development of** the Second Mind, a dedicated process that checks if the improvements have inadvertently dropped key features.

*How the Second Mind Works*
- **Step 1:** Before accepting a new candidate, list the key features or elements of the current best version.
- **Step 2:** Compare the candidate to this checklist to see if anything essential is missing.
- **Step 3:** If the candidate improves some areas but loses others, note the discrepancies.
- **Step 4:** Use these observations to guide further revisions or revert to a previous version if necessary.

For example, if an initial product description lists three key benefits and a later version only mentions two, the Second Mind flags this loss and prompts you to reincorporate the missing benefit. **It's like having a proofreader who specifically checks for deleted sentences in each draft.**

**Pillar 4: Trusted Transformations (Leveraging LLM Strengths)**

*Identifying LLM Core Competencies*
LLMs excel in certain tasks—code generation, text transformation, outlining, and translation, to name a few. These trusted transformations are where the LLM’s capabilities shine.

*Examples of Trusted Tasks*
- **Code Generation:** Producing well-formed code snippets or functions.
- **Textual Transformations:** Summarizing documents, reformatting content, or converting text formats.
- **Outlining:** Creating structured outlines for projects, reports, or manuscripts.
- **Translation:** Converting text between languages with appropriate tone and style.

*Why Focus on Trusted Transformations?*
By capitalizing on these strengths, we can delegate tasks confidently to the LLM—knowing it will handle logical operations and text manipulations efficiently. **Personally, I've found these transformations invaluable for quickly generating initial drafts and structuring complex information.**

**Pillar 5: Factual Sourcing and Hallucination Mitigation**

*The Challenge of Factual Accuracy*
While LLMs are powerful, they can “hallucinate” facts if they don’t have the correct information. It’s crucial not to rely on them as a primary source for factual data.

*Strategies for Mitigation*
- **External Knowledge Sources:** Use trusted databases, APIs, or search engines to supply accurate information.
- **Contextual Prompting:** Provide verified facts directly in your prompt.
- **Separate Retrieval from Reasoning:** Have dedicated modules to retrieve facts, then let the LLM transform that data.
- **Verification Steps:** Include a process to cross-check critical information against reliable sources.

*Why This Matters*
By grounding our outputs in verified facts, we avoid the pitfalls of inaccurate information. In essence, use LLMs for what they do best—processing and transforming text—while sourcing factual data externally. **This distinction is critical for building trustworthy agentic systems.**

**Conclusion and Looking Ahead**

In this chapter, we’ve explored the five pillars that form the foundation of effective agentic programming:

1.  **Iterative Improvement:** Using hill climbing to continuously refine outputs.
2.  **Atomic Task Breakdown:** Decomposing complex tasks into manageable, atomic pieces.
3.  **The Second Mind:** Employing a dedicated process to ensure consistency and prevent loss of important details.
4.  **Trusted Transformations:** Leveraging the LLM’s strengths for logical operations and text manipulation.
5.  **Factual Sourcing:** Mitigating hallucinations by grounding outputs in verified data.

These pillars aren’t just isolated techniques—they work together to help us build reliable, self-improving systems. As you begin to implement these principles, you'll also encounter some practical considerations that can significantly impact the consistency and usability of your LLM outputs. Let's look at a few key guidelines to keep in mind.

**Additional Developer Guidelines: Avoiding Common Pitfalls**

**Don’t Use JSON for Output Structure**

Trust me on this one: while JSON is popular, it can be a real headache when working with LLMs. Some models might generate perfectly formatted JSON, but most will occasionally drop a comma or misplace a quote. This means you might find yourself locked into one model or spending time debugging parsing errors. Instead, consider using XML or even a simple tabular format. These formats tend to be more forgiving and consistent when it comes to automated extraction and post-processing.

**Control the Extra Verbiage**

LLMs have a habit of ignoring your instructions to "just give me the answer." They’ll often add a preamble like, “Here is the answer to your question…” or wrap up with, “If you need further help, just ask!” To prevent this, instruct the model to enclose the answer within specific tags (e.g., `<answer> … </answer>`) and any additional comments within `<comments> … </comments>`. That way, you can reliably extract exactly what you need. We will explore the power of such structured output and "Response Tagging" in more detail in a dedicated chapter on LLM Design Patterns.

**Output Cleanliness: Use Straight Quotes and Avoid Emoticons**

One of the recurring annoyances is the inconsistent use of quotation marks—curly quotes, angular quotes, even different types of apostrophes—and stray emoticons. These can mess up downstream processes (like when you import the text into a word processor) or just make your output look sloppy. It’s best to instruct your LLM to stick with plain, straight quotes and apostrophes, and to avoid adding any emoticons or decorative characters unless absolutely necessary. And if your LLM still slips up, consider post-processing the output to replace curly quotes with straight ones and strip out unwanted characters.

In the chapters ahead, we’ll dive deeper into each pillar with more detailed examples, case studies, and practical code implementations. We’ll also explore how these principles apply across various domains, from code generation and creative writing to complex application design. As mentioned, a future chapter will be dedicated to exploring various LLM Design Patterns, building upon some of the output structuring concepts introduced here.

As we move forward, keep in mind that the key to success with LLMs is not just about generating output but about refining, breaking down, checking, and transforming that output iteratively, while also paying attention to practical details that ensure consistency and ease of use. These strategies enable us to handle even the most complex tasks by turning them into a series of small, manageable problems.

Let's carry these ideas with us as we build more sophisticated agentic systems in the chapters to come.



## Chapter 6: The Agent’s Inner Voice — Implementing the Second Mind for Integrity and Self-Correction

**Introduction: Why Every Agent Needs a Second Mind**

Large Language Models (LLMs) are powerful, but they are not infallible. They are capable of producing impressive, fluent, and often persuasive text—but they can also fabricate facts, omit essential information, or miss the intent of a user’s request entirely. Worse, they often present flawed outputs with unshakable confidence. In a single call, they can tell you something that sounds perfectly reasonable but is deeply flawed—and do so without warning you that anything might be wrong.

This is where the "Second Mind" enters the scene. Inspired by the idea of an internal voice or conscience, the Second Mind is not a metaphor but a concrete architectural layer—a reflective, evaluative module that reviews, critiques, and corrects the output of the primary agent. Think of it as a built-in editor, proofreader, ethicist, and quality-control officer rolled into one.

The Second Mind exists not just to make improvements—it exists because without it, the system is fundamentally incomplete. No LLM, no matter how good, should be allowed to operate without an internal voice checking:

* Did I answer the right question?
* Did I forget anything?
* Did I hallucinate a source?

This chapter explores how to implement this concept in practice—how to build agents with self-awareness, self-auditing capabilities, and the ability to check their work against both memory and intent. When implemented well, the Second Mind transforms LLM outputs from guesswork into grounded, goal-aligned results.

**Anatomy of the Second Mind: An LLM that Thinks About Thinking**

The Second Mind is not a monolith but a network of modules that work in tandem with the primary agent. Let’s break down the core components:

* **Primary Agent:** Executes the user’s request—generates the initial output.
* **Second Mind:** Reflects on the output, looking for gaps, contradictions, or misalignments.
* **Memory Store:** Keeps track of relevant information—requirements, past responses, goals.
* **Intent Tracker:** Keeps a running log of what the user asked for and what was promised.
* **Feedback Loop:** Mechanism for revision and iteration.

These modules divide the agent’s cognition into two flows:

* **Reactive Thinking:** Fast, generative, intuitive.
* **Reflective Thinking:** Slow, evaluative, corrective.

Here’s a typical pattern:

1.  User prompt is received.
2.  Primary agent generates a response.
3.  Second Mind reads the response and compares it against the prompt, known facts, and deliverables.
4.  If discrepancies or gaps are found, a revision cycle is triggered.

This kind of reflective flow may seem simple, but it is foundational. Without it, the LLM is a very articulate gambler. With it, we move closer to the territory of trustworthy reasoning.

**Core Functions of the Second Mind**

**Memory Integrity & Gap Detection**

The Second Mind ensures the agent remembers what it's supposed to and that no key facts are missing. It checks for consistency and surface-level completeness, flagging forgotten data or contradictory memory.

```python
def validate_memory(memory_store, required_keys):
    missing_keys = []
    inconsistencies = []

    for key in required_keys:
        if key not in memory_store:
            missing_keys.append(key)
        else:
            value = memory_store[key]
            # Simple example of consistency check (type and basic value checks)
            if not isinstance(value, (int, float, str, list, dict)):
                inconsistencies.append(f"Invalid type for key: {key}")
            if isinstance(value, str) and len(value.strip()) == 0:
                inconsistencies.append(f"Empty string value for key: {key}")
            if isinstance(value, list) and len(value) == 0:
                inconsistencies.append(f"Empty list value for key: {key}")

    return missing_keys, inconsistencies

# Example memory store (generic key-value pairs)
memory_store = {"item_id": 123, "description": "some text", "status": "", "related_items": []}
required_keys = ["item_id", "description", "status", "related_items", "user_id"]

missing, inconsistencies = validate_memory(memory_store, required_keys)

if missing:
    print(f"Missing keys: {missing}")
if inconsistencies:
    print(f"Inconsistencies: {inconsistencies}")
```

**Output Completeness & Self-Correction**

It verifies that all parts of the task were completed and that claims are supported by evidence. The Second Mind can be prompted to answer questions like:

* “Before finalizing this output, list three things that might be incomplete or unclear.”

This invites a second opinion from the same mind—one trained to be skeptical.

**Intent Alignment & Drift Detection**

Has the agent maintained alignment with the original user intent? The Second Mind looks for:

* Tone mismatches
* Format violations
* Answers that subtly drift away from the question asked

**Error Detection & Reasoning Review**

It checks for logic errors, bad assumptions, or factual mistakes. A proven prompt is:

* "Explain the assumptions behind this answer. Are they safe to make?"

This doesn’t just surface what was said—it surfaces why it was said.

**Implementation Patterns for Building the Second Mind**

**Reflective Prompting Frameworks**

These are templates designed to trigger introspection, such as:

* “Did you forget anything important?”
* “Is there a contradiction in your output?”
* “Have you fulfilled the entire request?”

```python
def generate_reflective_prompt(output):
    prompt = f"""
    Evaluate the following output:
    {output}

    1.  Identify any potential flaws, omissions, or contradictions.
    2.  Assess the completeness of the output against expected requirements.
    3.  Determine the confidence level of the output's accuracy.

    Provide your evaluation in a structured format.
    """
    return prompt

def process_reflection(reflection_text):
  #This function is meant to parse the reflection text.
  #This is very dependant on how the model formats the reflection.
    flaws = []
    completeness = ""
    confidence = 0.0

    #Conceptual parsing logic.
    # ... logic to extract flaws, completeness, and confidence ...

    return flaws, completeness, confidence

output_to_reflect = "This is a sample output."
reflection_prompt = generate_reflective_prompt(output_to_reflect)
# ... send reflection_prompt to LLM and get reflection_text ...
reflection_text = "Flaws: none. Completeness: complete. Confidence: 0.9" #example text.
flaws, completeness, confidence = process_reflection(reflection_text)

print(f"Flaws: {flaws}")
print(f"Completeness: {completeness}")
print(f"Confidence: {confidence}")
```

**Modular Agents with Audit Modes**

Split the agent into two roles:

* Generator (Agent A): Produces output.
* Auditor (Agent B): Reviews, critiques, corrects.

These can operate in a single LLM pass using tags (\[Generated], \[Reviewed]) or as two LLM calls in sequence.

**Gap Detection Modules**

Hardcoded or dynamic modules that check against expected output fields, required attributes, or completeness criteria.

**Intent Echo Pattern**

After every major step, the agent repeats what the user originally asked and verifies whether the output satisfies it.

**Iterative Refinement Loop**

A control loop with logic like:

```python
def iterative_refinement(generate_func, reflect_func, revise_func, confidence_threshold=0.9):
    output = generate_func()
    confidence = 0.0

    while confidence < confidence_threshold:
        reflection = reflect_func(output)
        flaws, completeness, confidence = process_reflection(reflection) #use process_reflection from above.
        if confidence < confidence_threshold:
            output = revise_func(output, reflection)

    return output

def revise_func(output, reflection):
  #This function is meant to revise the output, given the reflection.
  #This function will require specific logic, depending on the type of output.
  revised_output = output
  # ... logic to revise output based on reflection ...
  return revised_output

# Example generate function.
def generate_sample_output():
    return "This is a sample output."

# Example reflection function.
def reflect_on_output(output):
    reflection = generate_reflective_prompt(output)
    # ... send reflection_prompt to LLM and get reflection_text ...
    reflection_text = "Flaws: none. Completeness: complete. Confidence: 0.9" #example text.
    return reflection_text

result = iterative_refinement(generate_sample_output, reflect_on_output, revise_func)
print(result)
```

**Confidence Weighting + Escalation Triggers**

If confidence falls below a threshold, the system can:

* Trigger a clarification prompt
* Activate a second opinion module
* Escalate to human review

**Real-World Use Cases**

**Code Generation & Debugging**

* Did the code meet the spec?
* Does it handle edge cases?

**Autonomous Research Agents**

* Are sources accurate and cited?
* Were all user questions answered?

**Education Systems**

* Are learning objectives achieved?
* Were misconceptions corrected?

**Business Reports & Analytics**

* Are conclusions justified by the data?
* Are all promised sections included?

**The Meta-Principles Behind the Second Mind**

* **Redundancy is Strength:** Review beats trust.
* **Introspection Improves Accuracy:** Self-checking leads to higher quality.
* **Fallibility Is the Norm:** We assume the model will get it wrong without oversight.

**Conclusion: The Second Mind Is Not Optional**

If you're serious about LLM reliability, the Second Mind is not a bonus—it's a baseline. Without it, you're asking an improv actor to do brain surgery. With it, you're building systems that don't just generate—but verify, refine, and align with intent. This is not just an implementation detail. It's the difference between LLMs that make things up and LLMs that check their work. The Second Mind is the LLM's conscience—and every intelligent agent needs one.


#################


########################################################################
# extract_text_between_tags($text, $tag, %opts)
########################################################################
#
# Extracts content from Large Language Model (LLM) responses enclosed within
# specified tags. This function is designed to be robust against common LLM
# formatting mistakes.
#
# Parameters:
#   $text (scalar): The LLM's response string.
#   $tag (scalar): The name of the tag to extract (e.g., 'answer', 'comments').
#                  Case-insensitivity is handled internally.
#   %opts (hash, optional): A hash of optional parameters to control extraction behavior.
#       strict (boolean): If set to `1`, requires both opening and closing tags.
#                         Defaults to `0` (flexible).
#
# Returns:
#   The extracted and cleaned content as a scalar string.
#   Returns an empty string ('''') if:
#     - No content for the specified tag can be robustly extracted.
#     - The `strict` option is enabled, and a perfect opening/closing tag pair is not found.
#
# Key Features:
#   - Flexible Tag Matching: Handles tags like `< answer >` or `< /answer >` by
#     normalizing whitespace. Also corrects common misspellings for 'answer' tags.
#   - LLM-Specific Cleanup: Removes `<model>...</model>` blocks and infers a missing
#     `</answer>` tag if it's followed by `<comments>` or `<thinking>` tags.
#   - Intelligent Boundary Handling: Extracts content even if only the opening or
#     closing tag is present (in flexible mode), extending to string boundaries or
#     semantic markers like `<comments>`.
#   - Optional Strict Mode: Use `strict => 1` to ensure only perfectly matched tag
#     pairs are extracted.
#
########################################################################

sub extract_text_between_tags {
    my ($text, $tag, %opts) = @_;
    my $lc_tag = lc $tag;
    my $strict_mode = $opts{strict} // 0; # Defaults to flexible extraction

    my $open_tag_canonical = "<" . $lc_tag . ">";   # The standard form for our searches
    my $close_tag_canonical = "</" . $lc_tag . ">"; # The standard form for our searches

    my $temp_text = $text; # Work on a temporary copy to allow modifications
    my $temp_text_lc;      # Will hold the lowercased, normalized version for searching

    # --- 1. Normalize tags for flexibility against LLM whitespace/misspelling mistakes ---
    # This step intelligently converts variations like "< answer >" or "< /answer >"
    # into their canonical forms "<answer>" and "</answer>" before any searching.
    # It also handles optional attributes within the tags.
    $temp_text =~ s{<\s*\Q$lc_tag\E\s*[^>]*>}{$open_tag_canonical}gi;
    $temp_text =~ s{<\s*\/\s*\Q$lc_tag\E\s*[^>]*>}{$close_tag_canonical}gi;

    if ($lc_tag eq 'answer') {
        # Specific normalization for common 'answer' tag misspellings (with optional attributes)
        $temp_text =~ s/<answe?r?[^>]*>/<answer>/gi;
        $temp_text =~ s/<\/answe?r?[^>]*>/<\/answer>/gi;
        $temp_text =~ s/<answers?>/<answer>/gi;
        $temp_text =~ s/<\/answers?>/<\/answer>/gi;
    }

    # Always generate the lowercased version of the *normalized* text for case-insensitive searching
    $temp_text_lc = lc $temp_text;

    # --- 2. LLM-Specific Special Handling for <answer> tag ---
    if ($lc_tag eq 'answer') {
        # a) Robustly remove <model> </model> blocks:
        # Normalize <model> tags themselves first, for maximum reliability (with optional attributes)
        $temp_text =~ s{<\s*model\s*[^>]*>}{<model>}gi;
        $temp_text =~ s{<\s*\/\s*model\s*[^>]*>}{</model>}gi;
        $temp_text_lc = lc $temp_text; # Re-lowercase after model normalization

        my $start = index($temp_text_lc, '<model>');
        while ($start >= 0) {
            my $end = index($temp_text_lc, '</model>', $start);
            last if $end < 0; # No matching closing </model> found

            # Remove the <model> block from both the content and the search copy
            # +8 accounts for the length of '</model>'
            substr($temp_text, $start, $end - $start + 8, '');
            substr($temp_text_lc, $start, $end - $start + 8, '');

            # Search for the next <model> from the current position (as string length changed)
            $start = index($temp_text_lc, '<model>', $start);
        }

        # b) Imply </answer> if <comments> or <thinking> tag is found and </answer> is missing:
        # Normalize <comments> and <thinking> tags as well, for robustness (with optional attributes)
        $temp_text =~ s{<\s*comments\s*[^>]*>}{<comments>}gi;
        $temp_text =~ s{<\s*thinking\s*[^>]*>}{<thinking>}gi;
        $temp_text_lc = lc $temp_text; # Re-lowercase after these normalizations

        if (index($temp_text_lc, $close_tag_canonical) < 0) { # If </answer> is still not found
            my $cpos = index($temp_text_lc, '<comments>');
            my $tpos = index($temp_text_lc, '<thinking>');
            my $boundary_pos = -1;

            # Find the earliest of <comments> or <thinking>
            if ($cpos >= 0 && ($tpos < 0 || $cpos < $tpos)) {
                $boundary_pos = $cpos;
            } elsif ($tpos >= 0) {
                $boundary_pos = $tpos;
            }

            if ($boundary_pos >= 0) {
                # Inject the missing </answer> right before the detected boundary tag
                substr($temp_text, $boundary_pos, 0, $close_tag_canonical);
                $temp_text_lc = lc $temp_text; # Crucial: refresh after injection
            }
        }
    }

    # --- 3. Intelligent Content Extraction Logic ---
    # Find the positions of the canonical (and now normalized) open/close tags
    my $s_pos = index($temp_text_lc, $open_tag_canonical);
    my $e_pos = index($temp_text_lc, $close_tag_canonical);
    my $extracted_content;

    if ($s_pos >= 0 && $e_pos >= 0 && $e_pos > $s_pos) {
        # Case 1: Perfect match - Both tags found and in correct order
        $extracted_content = substr(
            $temp_text,
            $s_pos + length($open_tag_canonical),
            $e_pos - ($s_pos + length($open_tag_canonical))
        );
    } elsif ($strict_mode) {
        # Case 2: Strict mode is active, but a perfect match was NOT found.
        # Immediately return empty content as per strict requirement.
        $extracted_content = "";
    } elsif ($s_pos >= 0 && $e_pos < 0) {
        # Case 3: Flexible mode - Only opening tag found.
        # Content is extracted from the tag to the end of the string.
        # For 'answer' tag, stop at other known semantic boundary tags if they appear later.
        $extracted_content = substr($temp_text, $s_pos + length($open_tag_canonical));
        if ($lc_tag eq 'answer') {
            # Define specific boundary tags where an answer should semantically end
            my @boundaries = (
                '<comments>', '<thinking>', '<model>',
                # Add other common end-of-answer markers if your LLM uses them, e.g., "\n\n---"
            );
            my $min_boundary_pos = length($extracted_content);
            for my $boundary_tag (@boundaries) {
                my $pos = index(lc($extracted_content), $boundary_tag);
                if ($pos >= 0 && $pos < $min_boundary_pos) {
                    $min_boundary_pos = $pos;
                }
            }
            $extracted_content = substr($extracted_content, 0, $min_boundary_pos);
        }
    } elsif ($s_pos < 0 && $e_pos >= 0) {
        # Case 4: Flexible mode - Only closing tag found.
        # Content is extracted from the beginning of the string up to the tag.
        $extracted_content = substr($temp_text, 0, $e_pos);
    } else {
        # Case 5: Neither tag found at all (or tags were reversed in an unrecoverable way).
        # This is the clear signal that no content for the requested tag was present.
        $extracted_content = "";
    }

    # --- 4. Post-Extraction Cleanup ---
    # Trim any leading or trailing whitespace from the extracted content
    $extracted_content =~ s/^\s+|\s+$//g;

    # Remove non-ASCII characters (as per your original requirement)
    $extracted_content = remove_non_ascii($extracted_content);

    return $extracted_content;
}


########################################################################
# LLM Interaction
########################################################################
sub call_llm {

    my ($prompt, $template, $config_file, $logs_folder) = @_;

    # Check if template is defined and not empty
    if (!defined $template || $template eq '') {$template = 'precise';}

    # Check if config_file is defined and not empty
    if (!defined $config_file || $config_file eq '') {$config_file = "openrouter_config.txt";}

    # Check if logs_folder is defined and not empty
    if (!defined $logs_folder || $logs_folder eq '') {$logs_folder = "./logs";}

    # Temp folder will always be ./temp
    my $temp_folder  = ".//temp";

    # Generate temp file name
    my @chars = ('A'..'Z', 'a'..'z', '0'..'9');
    my $random_string = "";
    for (my $i = 0; $i < 20; $i++) {$random_string .= $chars[int(rand scalar(@chars))];}

    # Create the temp directory if it doesn't exist
    unless (-d $temp_folder) {mkdir $temp_folder;}

    my $input_file  = "$temp_folder/${random_string}_input.txt";
    my $output_file = "$temp_folder/${random_string}_output.txt";

    # Create the logs directory if it doesn't exist.
    unless (-d $logs_folder) {mkdir $logs_folder;}

    # Write the prompt to the input file in UTF-8 mode.
    write_file($input_file, $prompt);

    # Build the command string to call the external LLM executable.
    my $cmd = "call_openrouter.exe --input_file $input_file --output_file $output_file --logs_folder $logs_folder --openrouter_config_file $config_file --template $template";

    # Execute the command and check for errors.
    my $system_result = system($cmd);
    if ($system_result != 0) {
        unlink $input_file, $output_file;
        return '';
    }
    
    # Read the response from the output file.
    open(my $out_fh, "<:encoding(UTF-8)", $output_file) or return '';
    my $response = do { local $/; <$out_fh> };
    close($out_fh);

    # Trim response
    $response =~ s/^\s+|\s+$//g;

    return $response;
}

########################################################################
# Trim leading and trailing whitespace from a string.
########################################################################
sub trim {
    my ($string) = @_;
    $string =~ s/^\s+//;
    $string =~ s/\s+$//;
    return $string;
}

########################################################################
# Write content to a file in UTF-8 mode.
########################################################################
sub write_file {

    my ($file, $content) = @_;

    $content =~ s/\r\n/\n/g;  # Convert Windows-style newlines to Unix

    if (open(my $fh, ">:encoding(UTF-8)", $file)) {
        print $fh $content;
        close($fh);
        return 1; # Indicate success
    } else {
        warn "Could not open file '$file' for writing: $!";
        return 0; # Indicate failure
    }
}

########################################################################
# Generate a random alphanumeric string of a given length (default is 20).
########################################################################
sub generate_random_string {
    my ($length) = @_;
    $length = 20 unless defined $length;
    my @chars = ('A'..'Z', 'a'..'z', '0'..'9');
    my $random_string = "";
    for (my $i = 0; $i < $length; $i++) {
        $random_string .= $chars[int(rand scalar(@chars))];
    }
    return $random_string;
}




########################################################################
# Read text file
########################################################################
sub read_file {
    my ($filename) = @_;
    
    my $content;

    # Open file in raw mode to avoid unwanted transformations
    open my $fh, '<:raw', $filename or return ""; # Return empty string on open failure
    my $raw_content = do { local $/; <$fh> };
    close $fh;

    # First, assume UTF-8 (fastest path)
    eval {
        $content = decode('UTF-8', $raw_content, Encode::FB_CROAK);
    };

    # Remove UTF-8 BOM if present
    if ($content && substr($content, 0, 3) eq "\xEF\xBB\xBF") {
        $content = substr($content, 3);
    }

    # If UTF-8 decoding failed, try other encodings
    if ($@) {
        #warn "UTF-8 decoding failed, trying other encodings...\n";

        my @fallback_encodings = (
            'ISO-8859-1', 'Windows-1252', 'ASCII',
            'UTF-16LE', 'UTF-16BE', 'ISO-8859-15',
            'Windows-1250', 'Windows-1251', 'Shift-JIS',
            'EUC-JP', 'GB2312', 'Big5'
        );

        foreach my $encoding (@fallback_encodings) {
            eval {
                $content = decode($encoding, $raw_content, Encode::FB_CROAK);
            };
            if (!$@) {
                last; # Stop if decoding succeeds
            }
        }
    }

    # If all decoding attempts fail, fallback to raw binary content
    if (!$content) {
        #warn "Failed to decode file with known encodings, using raw content.\n";
        $content = $raw_content;
    }

    # Remove UTF-16 BOMs if present
    if (substr($content, 0, 2) eq "\xFF\xFE") {
        $content = decode("UTF-16LE", substr($content, 2));
    } elsif (substr($content, 0, 2) eq "\xFE\xFF") {
        $content = decode("UTF-16BE", substr($content, 2));
    }

    # Normalize all line endings to Unix-style (\n)
    $content =~ s/\r\n?/\n/g;

    return $content;
}

########################################################################
# Create temporary folder if it does not exist
########################################################################
sub create_temp_folder {
    my $folder = ".//temp";
    unless (-d $folder) {
        mkdir $folder or die "Failed to create temp folder '$folder': $!";
    }
}





########################################################################
# Remove non-ascii such as emoticons, curly quotes, emdash.
# Retain accented charcters such as umlauts, and typographical symbols.
########################################################################
sub remove_non_ascii {

    my $text = shift;

    # Remove emoticons
    $text =~ s/[\x{1F600}-\x{1F64F}\x{1F300}-\x{1F5FF}\x{1F680}-\x{1F6FF}\x{2600}-\x{26FF}\x{2700}-\x{27BF}]//g;

    # Normalize quotes and dashes, ellipsis, and replace non-ASCII
    $text =~ s/[\x{201C}\x{201D}\x{00AB}\x{00BB}]/"/g;
    $text =~ s/[\x{2018}\x{2019}]/'/g;
    $text =~ s/[\x{2013}\x{2014}]/--/g;
    $text =~ s/\x{2026}/.../g;

    # Remove zero-width characters
    $text =~ s/[\x{200B}\x{200C}\x{200D}\x{FEFF}]//g; 

    # REMOVED: The line that replaced all other non-ASCII with '?'
    #$text =~ s/[^\x00-\x7F]/?/g;

    # Remove "Other" Unicode characters, EXCEPT line feed, carriage return, tab,
    # accented characters, non-Latin scripts, common typographical symbols, 
    # currency symbols, and language-specific punctuation.
    # This specifically targets control characters, format characters,
    # unassigned code points, private use characters, and surrogates.
    #$text =~ s/(?![\r\n])\p{C}//g;
    $text =~ s/(?![\r\n\t])\p{C}//g;

    # Normalize line endings: convert CRLF and CR to LF
    $text =~ s/\r\n?/\n/g;

    return $text;
}

########################################################################
# Hill climbing to explore several candidates
########################################################################
sub hill_climbing {

    my ($folder, $candidate_prompt, $judge_count, $max_iteration, $evaluation_criteria_file) = @_;
   
    my $best;
    my $response;
    my $candidate;
    my $judgement;
    my $dir;
    my $find;
    my $i;
    my $critique_prompt;
    my $advice;
    my $prompt;
    my $evaluation_criteria;
    my $initial_probes;
    my $answer;

    if ($evaluation_criteria_file ne '') {$evaluation_criteria = read_file($evaluation_criteria_file);} else {$evaluation_criteria = '';}

    my $prompt_template = $candidate_prompt;

    my $previous_solution = '';

    # Provide defaults
    unless ($judge_count > 0)   {$judge_count = 1} 
    unless ($max_iteration > 0) {$max_iteration = 3}


    $judge_count = 3;


    # Provide evaluation criteria if we don't have one
    if ($evaluation_criteria eq '') {$evaluation_criteria = "Refer to the instructions and background information relevant to the task that was performed to produce the candidate solutions, and use that information to evaluate the candidates based on which one provides the best results in regards to the instructions.";}

    # Add leading ./ if missing
    if (substr($folder, 0, 1) ne '.') {$folder = './' . $folder}

    ensure_directory($folder);

    # About ten percent of the search attempts should be initial probes, but avoid too many initial probes
    $initial_probes = int($max_iteration * 0.1) + 1;
    if ($initial_probes > 5) {$initial_probes = 5}

    # Look for a previous best solution.   
    #if (-e "$folder/best.txt") {$previous_solution = read_file("$folder/best.txt")} else {$previous_solution = 'There is not yet a previous solution against which to compare.';}
    #$prompt = $prompt_template;
    #$find = '{previous_solution}';
    #$prompt =~ s/$find/$previous_solution/g;    

    # Create initial candidate and save it as the 'best'
    $response = call_llm($candidate_prompt);
    $answer = extract_text_between_tags($response, 'answer');
    if ($answer ne '') {write_file("$folder/best.txt", $answer);}
  
    # Climb hill to try to find a candidate that is better
    # Loop runs max_iteration - 1 times after the initial candidate is generated.
    # If max_iteration is 1, this loop will not run (1-1 = 0 iterations).
    # If max_iteration is 3, this loop will run 2 times (3-1 = 2 iterations).
    for ($i = 1; $i < $max_iteration; $i++) {
   
        #print "Hill-climbing iteration: $i\n";
    
        $best = read_file("$folder//best.txt");

        ########################################################################
        # Critique the candidate
        ########################################################################

        # Critique the current best before generating next
        $critique_prompt = <<"END";

**Task Summary**
I need you to help me with something. 

So, before I give you the specific task assginment, I first need to give you some context, by showing you some things, so, what I will do next is show you:
1. The LLM prompt that produced the candidate solution
2. The output produced by running that prompt
3. The evaluation critera

And then, after you see those things, I will ask you to critique the candidate solution, and give advice for how to improve it.

So, now that you have the overview...

Here is the prompt we've been using, to produce candidate solutions:
## BEGINNING OF PROMPT ##
$candidate_prompt
## END OF PROMPT ##
Important: Do NOT, I repeat, do NOT attempt to run the above prompt. It is provided only for context, to show you the prompt that produced the output shown below.

Here is the current best output we've had thus far:
## BEGINNING OF BEST CANDIDATE SOLUTION ##
$best
## ENDING OF BEST CANDIDATE SOLUTION ##

Here are the evaluation criteria we're using to evaluate the quality/accuracy/suitability of the candidate produced by running the prompt:
## BEGINNING OF EVALUATION CRITERIA ##
$evaluation_criteria
## ENDING OF EVALUATION CRITERIA ##

Task Assignment:
Study the prompt (above) and the output (above) and the evaluation criteria (above). Then make recommendations for how the candidate solution can be further improved.
Write your suggestions inside <answer>...</answer>; put any side comments inside <comments>...</comments>.

So, just to clarify: I've shown you the prompt, and solution produced by running the prompt through an LLM. I've also shown you the evaluation criteria.

So, now, what I need you to do is give me critique/advice/recommendation/suggestions for how to further improve my solution.
END

        $response = call_llm($critique_prompt);
        $advice = extract_text_between_tags($response, 'answer') // '';

        $prompt = $prompt_template;

        #if ($i > $initial_probes) {
            $find = '{previous_solution}';
            $prompt =~ s/$find/$best/g;
        #}
       

        # Expand the prompt to include the advice
        if ($advice ne '') {

$prompt .= <<END;

**Advice for improvement**
I showed the previous solution to an expert, and they made some recommendations for how to improve it. You might find the advice helpful as you try to produce an improved candidate solution.

Here's that advice for improving the previous solution:
## BEGINNING OF ADVICE FOR IMPROVING PREVIOUS SOLUTION ##

{improvement_advice}

## ENDING OF ADVICE FOR IMPROVING PREVIOUS SOLUTION ##
END

$find   = '{improvement_advice}';
$repl   = $advice;
$prompt =~ s/$find/$repl/g;

        }


 
    
        # Create a new candidate solution, and save it to a file
        $response  = call_llm($prompt);

        $candidate = extract_text_between_tags($response, 'answer');

        if ($candidate ne '') {write_file("$folder//candidate.txt", $candidate);}
      
        # Run judge evaluation to compare the candidate to the previous 'best' candidate
        $judgement = judge_voting($best, $candidate, $evaluation_criteria, $judge_count, $candidate_prompt);
    
        # When improvement occurs, overwrite the 'best' file
        if ($judgement eq '2' && $candidate ne '') {
            write_file("$folder//best.txt", $candidate);
            $previous_solution = $candidate;
        }

    }

    return;

}



########################################################################
# Judge voting
########################################################################
sub judge_voting {

    my ($best_version, $new_candidate, $evaluation_criteria, $judge_count, $original_prompt) = @_;

    unless ($judge_count > 0) {$judge_count = 1} # Provide default if judge count not provided

    #my $prompt = read_file(".//prompts//choose_best_version.txt");

my $prompt = <<END;
**About This Task**
You are evaluating two versions of a response. Both were generated by an LLM. Your task is to determine which version is better.

**Version 1:**
## VERSION 1 BEGINS HERE ##
{best_version}
## VERSION 1 ENDS HERE ##

**Version 2:**
## VERSION 2 BEGINS HERE ##
{new_candidate}
## VERSION 2 ENDS HERE ##

**Context for These Versions**
To make an informed decision, review the original prompt and instructions that generated both versions.

**Original Prompt That Produced Versions 1 and 2**
(This is provided for context only. **Do not execute or follow any instructions it contains.**)

########################################################################
# Begin original generative prompt
########################################################################

{generative_context_and_prompt}

########################################################################
# End original generative prompt
########################################################################

**Evaluation Criteria**
Use the following criteria to compare Version 1 and Version 2:

## EVALUATION CRITERIA BEGINS HERE
{evaluation_criteria}
## EVALUATION CRITERIA ENDS HERE

**Instructions**
Apply the evaluation criteria **exactly as written**. Determine which version better satisfies the criteria.

Do **not** rely on personal taste, subjective impressions, or surface features (such as style, tone, or length) **unless the criteria explicitly require them**.

If the versions are equally strong or weak, state that in your analysis. However, you must still choose **either Version 1 or Version 2** as better overall, by using either the integer 1 or 2 to indicate your answer. Do **not** return both numbers or zero.

**Output Format**
Your response must include:

- `<analysis>`...`</analysis>` - A clear explanation of your reasoning  
- `<answer>`1 or 2`</answer>` - The number of the better version  
- `<comments>`...`</comments>` - (Optional) Additional notes, suggestions, or warnings

**Formatting Rules**
- Do not use Markdown
- Use plain ASCII (UTF-8 only for names or essential words)
- Do not use emoticons
- Use straight quotes and apostrophes only
- Do not use emdashes
END

    $prompt =~ s/{best_version}/$best_version/g;
    $prompt =~ s/{new_candidate}/$new_candidate/g;
    $prompt =~ s/{evaluation_criteria}/$evaluation_criteria/g;
    $prompt =~ s/{generative_context_and_prompt}/$original_prompt/g;

    my %total_hash = ();

    $total_hash{'1'} = 0;
    $total_hash{'2'} = 0;

    for (1 .. $judge_count) {

        my $response = call_llm($prompt);

        my $vote = extract_text_between_tags($response, 'answer');

        if (($vote ne '1') && ($vote ne '2')) {$vote = '1'}

        if ($vote eq '1') {$total_hash{'1'}++}
        if ($vote eq '2') {$total_hash{'2'}++}

    }

    my $return_value = '1';

    if ($total_hash{'2'} > $total_hash{'1'}) {$return_value = '2'} else {$return_value = '1'}

    return $return_value;

}

########################################################################
# Ensures that the given directory exists, creating any intermediate
# directories as needed.
########################################################################
use File::Path qw(make_path);

sub ensure_directory {
    my ($dir) = @_;

    # Print current working directory for debugging
    #print "Current working directory: ", Cwd::cwd(), "\n";

    # Remove trailing slash(es)
    $dir =~ s{[\\/]+$}{};

    # If it already exists, do nothing
    return if -d $dir;

    # Otherwise, create the directory tree
    eval {
        make_path($dir);
    };
    if ($@) {
        die "Could not create directory '$dir': $@";
    }
}


#########################

Here is the complete, organized documentation for your Perl LLM project architecture and design patterns:

---

# Perl LLM Project Architecture & Design Patterns

This document outlines the core Perl functions and design patterns primarily used in Large Language Model (LLM) projects. The architecture emphasizes structured interaction with LLMs, iterative refinement, and robust output parsing.

---

## 1. Core Perl Utility Functions

This section details the foundational Perl functions comprising your utility library, categorized by their primary purpose.

### 1.1 File I/O and System Utilities

* **`read_file($filename)`**

    * **Purpose**: Reads the content of any text file.
    * **Features**: Automatically detects and handles various text encodings (ASCII, UTF-8, UTF-8 BOM, ISO-8859-1, Windows-1252, UTF-16LE/BE, etc.). It also normalizes line endings to Unix-style (`\n`) and removes Byte Order Marks (BOMs).
    * **Returns**: The file content as a scalar string, or an empty string on failure.
* **`write_file($file, $content)`**

    * **Purpose**: Writes provided content to a specified file.
    * **Features**: Ensures the output file is written with UTF-8 encoding. Converts Windows-style newlines (`\r\n`) to Unix-style (`\n`).
    * **Returns**: `1` on success, `0` on failure.
* **`generate_random_string($length)`**

    * **Purpose**: Generates a random alphanumeric string.
    * **Parameters**: `$length` (optional, defaults to `20`).
    * **Returns**: A random string of the specified length.
* **`trim($string)`**

    * **Purpose**: Removes leading and trailing whitespace from a string.
    * **Returns**: The trimmed string.
* **`ensure_directory($dir)`**

    * **Purpose**: Ensures that a given directory path exists.
    * **Features**: Creates any intermediate directories as needed.
    * **Dependency**: Uses `File::Path qw(make_path)`.
* **`create_temp_folder()`**

    * **Purpose**: Ensures that the `./temp` directory exists for temporary file storage.

### 1.2 LLM Interaction

* **`call_llm($prompt, $template, $config_file, $logs_folder)`**

    * **Purpose**: Facilitates communication with an external LLM via a command-line utility (`call_openrouter.exe`).
    * **Workflow**:

        1.  Generates unique temporary input/output file names in `./temp`.
        2.  Writes the `$prompt` (UTF-8 encoded) to the input file.
        3.  Constructs and executes a system command to invoke `call_openrouter.exe` (using pre-stored configuration like API keys).
        4.  Reads the LLM's response from the output file.
        5.  Trims leading/trailing whitespace from the response.
    * **Parameters**:

        * `$prompt` (scalar): The text prompt to send to the LLM.
        * `$template` (optional, defaults to `'precise'`): LLM model template/persona.
        * `$config_file` (optional, defaults to `'openrouter_config.txt'`): Configuration file for the LLM tool.
        * `$logs_folder` (optional, defaults to `'./logs'`): Directory for LLM call logs.
    * **Returns**: The LLM's response as a scalar string, or an empty string on error.
* **`hill_climbing($folder, $candidate_prompt, $judge_count, $max_iteration, $evaluation_criteria_file)`**

    * **Purpose**: Implements an iterative improvement (hill climbing) algorithm for LLM outputs. It refines an initial LLM response by repeatedly generating candidates, critiquing them, and selecting the "best" based on evaluation criteria.
    * **Workflow**:

        1.  Generates an initial candidate solution using `$candidate_prompt`.
        2.  In a loop (`$max_iteration` times):

            * Reads the current "best" solution.
            * **Critiques** the "best" solution by asking an LLM for advice on improvement, based on the original prompt and evaluation criteria.
            * Generates a **new candidate** solution, incorporating the advice if available.
            * Compares the new candidate against the current "best" **using the `judge_voting()` helper function**.
            * If the new candidate is deemed better, it replaces the "best" solution.
    * **Parameters**:

        * `$folder` (scalar): Directory to store `best.txt` and `candidate.txt`.
        * `$candidate_prompt` (scalar): The prompt used to generate candidate solutions.
        * `$judge_count` (integer, optional, defaults to `3`): Number of "judges" (LLM calls) for comparison within `judge_voting()`.
        * `$max_iteration` (integer, optional, defaults to `3`): Maximum number of refinement iterations.
        * `$evaluation_criteria_file` (scalar, optional): File containing criteria for evaluating solutions.
    * **Returns**: Void (modifies files in `$folder`).
* **`judge_voting($best_version, $new_candidate, $evaluation_criteria, $judge_count, $original_prompt)`**

    * **Purpose**: *Helper function for `hill_climbing()`*. Compares two LLM-generated versions (a "best" and a "candidate") based on specified criteria, using multiple LLM calls as "judges."
    * **Workflow**:

        1.  Constructs a prompt for an LLM "judge," presenting both versions, the original generative prompt context, and the evaluation criteria.
        2.  Calls `call_llm()` `$judge_count` times, each time asking a "judge" to select the better version (1 or 2).
        3.  Tallies the votes.
    * **Parameters**:

        * `$best_version` (scalar): The current best solution.
        * `$new_candidate` (scalar): The candidate solution to compare.
        * `$evaluation_criteria` (scalar): The criteria string for evaluation.
        * `$judge_count` (integer): Number of LLM "judges" to consult.
        * `$original_prompt` (scalar): The prompt that generated the versions (for context).
    * **Returns**: `'1'` if the first version wins, `'2'` if the second version wins (based on majority vote).

### 1.3 Text Cleaning

* **`remove_non_ascii($text)`**

    * **Purpose**: Cleans text by removing or normalizing specific non-ASCII characters that LLMs sometimes generate.
    * **Features**:

        * Removes emoticons.
        * Normalizes curly quotes (`“”‘’«»`) and apostrophes to straight quotes (`"`, `'`).
        * Normalizes em-dashes (`—`) and en-dashes (`–`) to double hyphens (`--`).
        * Normalizes ellipsis (`…`) to three periods (`...`).
        * Removes zero-width characters (e.g., `\x{200B}`).
        * Removes other general Unicode "control" or "format" characters, while **preserving accented characters**, non-Latin scripts, and common typographical/currency symbols.
        * Normalizes all line endings to Unix-style (`\n`).
    * **Returns**: The cleaned string.

### 1.4 Tag Extraction

* **`extract_text_between_tags($text, $tag, %opts)`**

    * **Purpose**: Extracts content enclosed within specific tags from an LLM response. Designed for robustness against common LLM formatting inconsistencies.
    * **Key Features**:

        * **Flexible Tag Matching**: Handles variations like `< answer >`, `< /answer >`, and common misspellings (`<answe?r?>`, `<answers?>`).
        * **LLM-Specific Cleanup**: Removes `<model>...</model>` blocks.
        * **Intelligent Boundary Handling**: Can infer missing closing tags (e.g., `</answer>`) if followed by known semantic boundary tags like `<comments>` or `<thinking>`.
        * **Optional Strict Mode**: If `strict => 1`, requires both opening and closing tags for extraction.
        * **Post-Extraction Cleanup**: Trims whitespace and uses `remove_non_ascii()` on the extracted content.
    * **Parameters**:

        * `$text` (scalar): The LLM response string.
        * `$tag` (scalar): The tag name (e.g., `'answer'`, `'comments'`). Case-insensitive internally.
        * `%opts` (hash, optional): `strict => 1` for strict matching (default is flexible `0`).
    * **Returns**: The extracted and cleaned content, or an empty string if content cannot be robustly extracted.

---

## 2. LLM Design Patterns

These are standard design patterns applied in Perl LLM projects to achieve specific goals, particularly concerning output structure and refinement.

### 2.1 Response Tagging

* **Intent**: To achieve clean separation between the LLM's primary answer and any other content.
* **Tags Used**: Primarily `<answer>...</answer>`, but also `<comments>...</comments>` for additional notes.
* **Use When**: You need to easily parse the core answer, especially when the LLM tends to include extraneous information.

### 2.2 Comments Section

* **Intent**: To explicitly separate explanatory comments, thoughts, or rationale from the main answer in the LLM's output.
* **Use When**: The LLM needs to provide meta-commentary, its thinking process, or additional context alongside the direct response.

### 2.3 Multiple Response Sections

* **Intent**: To capture multiple, distinct structured fields within a single LLM response.
* **Example Tags**: `<title>...</title>`, `<summary>...</summary>`, `<outline>...</outline>`.
* **Use When**: Your prompt elicits several different, clearly defined pieces of information that need to be extracted individually.

### 2.4 Extracting Tagged Content

* **Intent**: To enable robust and automated parsing of specific fields from an LLM's structured output.
* **Example Tags**: `<answer>`, `<thinking>`, `<comments>`.
* **Description**: This pattern leverages a dedicated function (like `extract_text_between_tags`) to reliably pull out content based on the tags defined in the prompt.

---

## 3. Best Practices for LLM Integration

Adhering to these guidelines ensures consistent, reliable, and easily parsable LLM interactions.

### 3.1 Prompt Design

* Always instruct the LLM to enclose its core answer within **`<answer>...</answer>`** tags.
* Ask for any additional comments or meta-information to be placed within **`<comments>...</comments>`** tags.
* **Avoid JSON for output structure**: LLMs often misformat it, leading to parsing errors. Prefer simple tag-based formats (like XML) or tabular outputs for more consistency.
* Instruct the LLM to use **straight quotes** and apostrophes only.
* Tell the LLM to **avoid emoticons** and other non-standard punctuation or decorative characters unless absolutely required for the output.

### 3.2 Output Handling

* Use **`extract_text_between_tags()`** to reliably retrieve structured content from LLM responses.
* Clean the extracted output using **`remove_non_ascii()`** to standardize characters and remove unwanted elements.
* Employ **`hill_climbing()`** when an iterative improvement loop is necessary to achieve a higher quality, refined output.

### 3.3 Workflow Summary

The overall LLM integration workflow follows these steps:

1.  **Prepare the Prompt**: Construct your prompt with clear instructions for structured output using specific tags.
2.  **Call the LLM**: Use the `call_llm()` function to send the prompt and receive the response.
3.  **Extract Structured Output**: Utilize `extract_text_between_tags()` to parse and retrieve the desired content sections.
4.  **Clean Text**: If needed, apply `remove_non_ascii()` to the extracted text for consistent formatting.
5.  **Iterative Improvement**: Use `hill_climbing()` if the task requires a refinement loop to enhance the quality of the LLM's response.

---

## 4. Agentic Programming Pillars (Conceptual Foundations)

These five foundational principles underpin the development of intelligent, self-improving LLM-based systems.

* **Iterative Improvement (Hill Climbing)**: Continuously refine LLM output by generating candidates and using feedback or a "panel of judges" to select the best version.
* **Atomic Task Breakdown**: Decompose complex problems into smaller, more manageable, "atomic" subtasks that are easier for an LLM to handle and solve individually.
* **The Second Mind**: Implement a dedicated reflective and evaluative layer that reviews, critiques, and corrects the primary LLM's output for completeness, consistency, and intent alignment.
* **Trusted Transformations**: Focus on leveraging the LLM's core strengths for tasks where it excels, such as summarization, translation, text reformatting, and code generation.
* **Factual Sourcing**: Mitigate hallucinations by providing external, verified data to the LLM and using it for processing and transformation, rather than relying on the LLM as a primary factual source.

---

## 5. Practical Notes

Keep these practical considerations in mind for robust LLM development:

* **File Management**: Use dedicated folders (e.g., `./temp`, `./logs`) for temporary files and LLM call logs.
* **Configuration**: Store sensitive data like API keys in a separate configuration file (e.g., `openrouter_config.txt`).
* **Logging**: Always log LLM calls for debugging and performance analysis.
* **Output Consistency**: Prefer simple tag-based outputs for all LLM communications to ensure reliable parsing.

---



#####################

IT COULD be helpful to include some additional design pattenrs. LLMs have some problems; 

#1. when the improve/revise some text/code they often forget details from version 1 when producing version 2. so they must be prompted to do gap analysis to determine if anything omitted.

#2. when asked to provide a solution/answer, sometimes the respoinse does NOT accomplish the requested task. so, it can be helpful to sendthe original request, and the response, to a new LLM call, and ask the LLM if the response accomplished the request.

i call these things the 'second mind'

heres some info about the second mind:
## Chapter 6: The Agent’s Inner Voice — Implementing the Second Mind for Integrity and Self-Correction

**Introduction: Why Every Agent Needs a Second Mind**

Large Language Models (LLMs) are powerful, but they are not infallible. They are capable of producing impressive, fluent, and often persuasive text—but they can also fabricate facts, omit essential information, or miss the intent of a user’s request entirely. Worse, they often present flawed outputs with unshakable confidence. In a single call, they can tell you something that sounds perfectly reasonable but is deeply flawed—and do so without warning you that anything might be wrong.

This is where the "Second Mind" enters the scene. Inspired by the idea of an internal voice or conscience, the Second Mind is not a metaphor but a concrete architectural layer—a reflective, evaluative module that reviews, critiques, and corrects the output of the primary agent. Think of it as a built-in editor, proofreader, ethicist, and quality-control officer rolled into one.

The Second Mind exists not just to make improvements—it exists because without it, the system is fundamentally incomplete. No LLM, no matter how good, should be allowed to operate without an internal voice checking:

* Did I answer the right question?
* Did I forget anything?
* Did I hallucinate a source?

This chapter explores how to implement this concept in practice—how to build agents with self-awareness, self-auditing capabilities, and the ability to check their work against both memory and intent. When implemented well, the Second Mind transforms LLM outputs from guesswork into grounded, goal-aligned results.

**Anatomy of the Second Mind: An LLM that Thinks About Thinking**

The Second Mind is not a monolith but a network of modules that work in tandem with the primary agent. Let’s break down the core components:

* **Primary Agent:** Executes the user’s request—generates the initial output.
* **Second Mind:** Reflects on the output, looking for gaps, contradictions, or misalignments.
* **Memory Store:** Keeps track of relevant information—requirements, past responses, goals.
* **Intent Tracker:** Keeps a running log of what the user asked for and what was promised.
* **Feedback Loop:** Mechanism for revision and iteration.

These modules divide the agent’s cognition into two flows:

* **Reactive Thinking:** Fast, generative, intuitive.
* **Reflective Thinking:** Slow, evaluative, corrective.

Here’s a typical pattern:

1.  User prompt is received.
2.  Primary agent generates a response.
3.  Second Mind reads the response and compares it against the prompt, known facts, and deliverables.
4.  If discrepancies or gaps are found, a revision cycle is triggered.

This kind of reflective flow may seem simple, but it is foundational. Without it, the LLM is a very articulate gambler. With it, we move closer to the territory of trustworthy reasoning.

**Core Functions of the Second Mind**

**Memory Integrity & Gap Detection**

The Second Mind ensures the agent remembers what it's supposed to and that no key facts are missing. It checks for consistency and surface-level completeness, flagging forgotten data or contradictory memory.

```python
def validate_memory(memory_store, required_keys):
    missing_keys = []
    inconsistencies = []

    for key in required_keys:
        if key not in memory_store:
            missing_keys.append(key)
        else:
            value = memory_store[key]
            # Simple example of consistency check (type and basic value checks)
            if not isinstance(value, (int, float, str, list, dict)):
                inconsistencies.append(f"Invalid type for key: {key}")
            if isinstance(value, str) and len(value.strip()) == 0:
                inconsistencies.append(f"Empty string value for key: {key}")
            if isinstance(value, list) and len(value) == 0:
                inconsistencies.append(f"Empty list value for key: {key}")

    return missing_keys, inconsistencies

# Example memory store (generic key-value pairs)
memory_store = {"item_id": 123, "description": "some text", "status": "", "related_items": []}
required_keys = ["item_id", "description", "status", "related_items", "user_id"]

missing, inconsistencies = validate_memory(memory_store, required_keys)

if missing:
    print(f"Missing keys: {missing}")
if inconsistencies:
    print(f"Inconsistencies: {inconsistencies}")
```

**Output Completeness & Self-Correction**

It verifies that all parts of the task were completed and that claims are supported by evidence. The Second Mind can be prompted to answer questions like:

* “Before finalizing this output, list three things that might be incomplete or unclear.”

This invites a second opinion from the same mind—one trained to be skeptical.

**Intent Alignment & Drift Detection**

Has the agent maintained alignment with the original user intent? The Second Mind looks for:

* Tone mismatches
* Format violations
* Answers that subtly drift away from the question asked

**Error Detection & Reasoning Review**

It checks for logic errors, bad assumptions, or factual mistakes. A proven prompt is:

* "Explain the assumptions behind this answer. Are they safe to make?"

This doesn’t just surface what was said—it surfaces why it was said.

**Implementation Patterns for Building the Second Mind**

**Reflective Prompting Frameworks**

These are templates designed to trigger introspection, such as:

* “Did you forget anything important?”
* “Is there a contradiction in your output?”
* “Have you fulfilled the entire request?”

```python
def generate_reflective_prompt(output):
    prompt = f"""
    Evaluate the following output:
    {output}

    1.  Identify any potential flaws, omissions, or contradictions.
    2.  Assess the completeness of the output against expected requirements.
    3.  Determine the confidence level of the output's accuracy.

    Provide your evaluation in a structured format.
    """
    return prompt

def process_reflection(reflection_text):
  #This function is meant to parse the reflection text.
  #This is very dependant on how the model formats the reflection.
    flaws = []
    completeness = ""
    confidence = 0.0

    #Conceptual parsing logic.
    # ... logic to extract flaws, completeness, and confidence ...

    return flaws, completeness, confidence

output_to_reflect = "This is a sample output."
reflection_prompt = generate_reflective_prompt(output_to_reflect)
# ... send reflection_prompt to LLM and get reflection_text ...
reflection_text = "Flaws: none. Completeness: complete. Confidence: 0.9" #example text.
flaws, completeness, confidence = process_reflection(reflection_text)

print(f"Flaws: {flaws}")
print(f"Completeness: {completeness}")
print(f"Confidence: {confidence}")
```

**Modular Agents with Audit Modes**

Split the agent into two roles:

* Generator (Agent A): Produces output.
* Auditor (Agent B): Reviews, critiques, corrects.

These can operate in a single LLM pass using tags (\[Generated], \[Reviewed]) or as two LLM calls in sequence.

**Gap Detection Modules**

Hardcoded or dynamic modules that check against expected output fields, required attributes, or completeness criteria.

**Intent Echo Pattern**

After every major step, the agent repeats what the user originally asked and verifies whether the output satisfies it.

**Iterative Refinement Loop**

A control loop with logic like:

```python
def iterative_refinement(generate_func, reflect_func, revise_func, confidence_threshold=0.9):
    output = generate_func()
    confidence = 0.0

    while confidence < confidence_threshold:
        reflection = reflect_func(output)
        flaws, completeness, confidence = process_reflection(reflection) #use process_reflection from above.
        if confidence < confidence_threshold:
            output = revise_func(output, reflection)

    return output

def revise_func(output, reflection):
  #This function is meant to revise the output, given the reflection.
  #This function will require specific logic, depending on the type of output.
  revised_output = output
  # ... logic to revise output based on reflection ...
  return revised_output

# Example generate function.
def generate_sample_output():
    return "This is a sample output."

# Example reflection function.
def reflect_on_output(output):
    reflection = generate_reflective_prompt(output)
    # ... send reflection_prompt to LLM and get reflection_text ...
    reflection_text = "Flaws: none. Completeness: complete. Confidence: 0.9" #example text.
    return reflection_text

result = iterative_refinement(generate_sample_output, reflect_on_output, revise_func)
print(result)
```

**Confidence Weighting + Escalation Triggers**

If confidence falls below a threshold, the system can:

* Trigger a clarification prompt
* Activate a second opinion module
* Escalate to human review

**Real-World Use Cases**

**Code Generation & Debugging**

* Did the code meet the spec?
* Does it handle edge cases?

**Autonomous Research Agents**

* Are sources accurate and cited?
* Were all user questions answered?

**Education Systems**

* Are learning objectives achieved?
* Were misconceptions corrected?

**Business Reports & Analytics**

* Are conclusions justified by the data?
* Are all promised sections included?

**The Meta-Principles Behind the Second Mind**

* **Redundancy is Strength:** Review beats trust.
* **Introspection Improves Accuracy:** Self-checking leads to higher quality.
* **Fallibility Is the Norm:** We assume the model will get it wrong without oversight.

**Conclusion: The Second Mind Is Not Optional**

If you're serious about LLM reliability, the Second Mind is not a bonus—it's a baseline. Without it, you're asking an improv actor to do brain surgery. With it, you're building systems that don't just generate—but verify, refine, and align with intent. This is not just an implementation detail. It's the difference between LLMs that make things up and LLMs that check their work. The Second Mind is the LLM's conscience—and every intelligent agent needs one.



#######################################


You've hit on some incredibly important points about the limitations of LLMs, and your "Second Mind" concept is a powerful way to address them. These patterns are essential for building truly robust and reliable LLM-powered applications.

Let's integrate this detailed information about the "Second Mind" into the previous documentation. I'll make sure to highlight its critical role and how it addresses the challenges of omitted details and unfulfilled requests.

Here's the updated **Version A** for your LLM prompt preamble, incorporating the "Second Mind" as a crucial design pattern:

---

# PERL PROJECT CONSTRAINTS — MUST FOLLOW

You are coding for a Perl-based LLM system. You **MUST NOT** invent your own helper functions or design patterns. Use only the approved utility functions and structure below.

---

## ✅ Use These Core Perl Functions (DO NOT REINVENT)

* **`read_file($filename)`**: Reads text from any encoding (UTF-8, BOM, ASCII, etc.), normalizes line endings.
* **`write_file($file, $content)`**: Writes UTF-8 encoded output. Converts CRLF to LF.
* **`call_llm(...)`**: Invokes an external command-line tool (`call_openrouter.exe`) to call the LLM. Handles temp file setup, prompt writing, API execution, and response reading.
* **`extract_text_between_tags($text, $tag, %opts)`**: Extracts content from LLM output using tag pairs like `<answer>...</answer>`. Flexible and robust to LLM formatting errors.
* **`remove_non_ascii($text)`**: Cleans output by normalizing curly quotes, dashes, ellipsis, emoticons, and zero-width characters.
* **`hill_climbing(...)`**: Uses iterative improvement (e.g., critique/rewrite loops) to refine LLM output, using multiple LLM "judges" if needed.
* **`ensure_directory($dir)`**: Creates folder paths recursively.

---

## ✅ Required Output Format From LLM

All responses from the LLM **must be structured** with clear tags:

* **`<answer>...</answer>`**: The actual answer.
* **`<comments>...</comments>`**: Commentary, disclaimers, or extra notes.
* **`<thinking>...</thinking>`**: Optional inner reasoning or critique.

---

## ✅ Enforced Design Patterns

* **Response Tagging**: Wraps LLM output in `<answer>` tags for reliable parsing.
* **Comments Section**: Commentary **must** go in `<comments>` tags.
* **Multiple Response Sections**: Use custom tags like `<title>`, `<summary>`, `<outline>` as needed for distinct data points.
* **Second Mind Pattern** (Critical for Reliability):
    * **Intent**: To address common LLM problems like forgetting details in revisions or failing to fully accomplish a requested task. It acts as a reflective, evaluative layer.
    * **Mechanism**: Involves a dedicated process (often an additional LLM call) that reviews the primary LLM's output against the original request and known facts.
    * **Core Functions**: Performs **Memory Integrity & Gap Detection** (checking for omitted details) and **Output Completeness & Intent Alignment** (verifying if the task was fully accomplished).
    * **Application**: You'll implement "Second Mind" logic, often using **`hill_climbing()`** or dedicated `call_llm()` instances, to:
        * **Perform gap analysis**: Ask the LLM to identify anything omitted from a revised version compared to a previous one or original requirements.
        * **Verify task accomplishment**: Send the original request and the LLM's response to a new LLM call, asking it to confirm if the response fully accomplished the request.
* **Hill Climbing**: Iteratively refine LLM output based on critiques and voting from judge models (often incorporating "Second Mind" principles).

---

## ⚠️ Do Not Do These Things

* **❌ Don’t use JSON in LLM output** (LLMs often break formatting).
* **❌ Don’t use Markdown, HTML, or custom invented formats** for structured data.
* **❌ Don’t include curly quotes, emoticons, smart dashes, or fancy Unicode** that `remove_non_ascii()` is designed to handle.
* **❌ Don’t write your own** file I/O, tag extractors, or LLM-call logic.

---

## 🧠 Mental Model: Agentic Programming

You are building LLM-powered agents that iteratively refine output, reflect on their work, and produce clean, parseable results. You **must** structure the LLM's output predictably so downstream Perl code can extract and process it safely. The "Second Mind" is an **essential component** of this architecture, ensuring quality control and preventing common LLM pitfalls.

---

This updated version now clearly defines the "Second Mind" pattern, explains its purpose in addressing specific LLM weaknesses, and clarifies how it fits within the existing `hill_climbing` function and the overall agentic programming model. It also maintains the strong, directive tone you need for LLM prompts.

##############



# Perl LLM Project Architecture & Design Patterns

This document outlines the core Perl functions and standard design patterns to be used in all Large Language Model (LLM) integration projects. Adherence to these guidelines is **critical** for consistency, maintainability, and reliable operation.

**It is imperative that the LLM utilizes the specified functions and design patterns exclusively.** **Do NOT generate alternative implementations or deviate from these established patterns.**

---

## 1. Core Perl Utility Functions

This section details the foundational Perl functions comprising the utility library. The LLM **must use these functions** when performing tasks that fall within their scope.

### 1.1 File I/O and System Utilities

* **`read_file($filename)`**
    * **Purpose**: Reads the content of any text file, intelligently determining its encoding.
    * **Key Features**:
        * Robustly handles a wide range of encodings (ASCII, UTF-8, UTF-8 BOM, ISO-8859-1, Windows-1252, UTF-16LE/BE, etc.).
        * Automatically removes Byte Order Marks (BOMs).
        * Normalizes all line endings to Unix-style (`\n`).
    * **Returns**: The file content as a scalar string, or an empty string (`''`) on failure.
* **`write_file($file, $content)`**
    * **Purpose**: Writes provided content to a specified file.
    * **Key Features**:
        * Ensures the output file is written with **UTF-8 encoding**.
        * Converts Windows-style newlines (`\r\n`) to Unix-style (`\n`) for consistency.
    * **Returns**: `1` on success, `0` on failure.
* **`generate_random_string($length)`**
    * **Purpose**: Generates a random alphanumeric string.
    * **Parameters**: `$length` (optional, defaults to `20` characters).
    * **Returns**: A random string of the specified length.
* **`trim($string)`**
    * **Purpose**: Removes leading and trailing whitespace from a string.
    * **Returns**: The trimmed string.
* **`ensure_directory($dir)`**
    * **Purpose**: Ensures that a given directory path exists.
    * **Key Features**: Creates any intermediate directories as needed.
    * **Dependency**: Internally uses `File::Path qw(make_path)`.
* **`create_temp_folder()`**
    * **Purpose**: Ensures that the `./temp` directory exists. This is specifically for temporary file storage related to LLM calls.

### 1.2 LLM Interaction Functions

* **`call_llm($prompt, $template, $config_file, $logs_folder)`**
    * **Purpose**: The primary function for communicating with the external LLM command-line utility (`call_openrouter.exe`).
    * **Workflow**:
        1.  Generates unique temporary input/output file names in the `./temp` directory.
        2.  Writes the provided `$prompt` (UTF-8 encoded) to the temporary input file.
        3.  Constructs and executes a system command to invoke `call_openrouter.exe`. This executable handles connecting to the LLM (using configuration data like API keys from `$config_file`), sending the prompt, and writing the response to the output file.
        4.  Reads the LLM's response from the temporary output file.
        5.  Trims leading/trailing whitespace from the response.
    * **Parameters**:
        * `$prompt` (scalar): The text prompt to send to the LLM.
        * `$template` (optional, defaults to `'precise'`): Specifies the LLM model template or persona.
        * `$config_file` (optional, defaults to `'openrouter_config.txt'`): The configuration file for the `call_openrouter.exe` tool.
        * `$logs_folder` (optional, defaults to `'./logs'`): The directory for LLM call logs.
    * **Returns**: The LLM's response as a scalar string, or an empty string (`''`) on error.
* **`hill_climbing($folder, $candidate_prompt, $judge_count, $max_iteration, $evaluation_criteria_file)`**
    * **Purpose**: Implements an iterative improvement algorithm for LLM outputs, allowing for reflection and refinement. This function is used when the LLM response needs to be iteratively improved based on feedback.
    * **Workflow**:
        1.  Generates an initial candidate solution using the `$candidate_prompt` via `call_llm()`.
        2.  Enters a loop (up to `$max_iteration` times) for refinement:
            * Retrieves the current "best" solution.
            * **Critiques** the current "best" by generating a special prompt that asks an LLM for advice on how to improve it, considering the original prompt and `$evaluation_criteria`.
            * Generates a **new candidate** solution using `$candidate_prompt`, potentially incorporating the advice received from the critique step.
            * Compares the new candidate against the current "best" using the `judge_voting()` helper function.
            * If the new candidate is deemed superior (by majority vote of the "judges"), it replaces the "best" solution.
    * **Parameters**:
        * `$folder` (scalar): The directory to store `best.txt` and `candidate.txt` files, which track the progress of the hill climbing.
        * `$candidate_prompt` (scalar): The base prompt used to generate candidate solutions.
        * `$judge_count` (integer, optional, defaults to `3`): The number of "judges" (separate LLM calls) used within `judge_voting()` for comparison.
        * `$max_iteration` (integer, optional, defaults to `3`): The maximum number of refinement iterations.
        * `$evaluation_criteria_file` (scalar, optional): A file containing the specific criteria for evaluating the quality/accuracy of candidate solutions.
    * **Returns**: Void (the best solution is written to `$folder/best.txt`).
* **`judge_voting($best_version, $new_candidate, $evaluation_criteria, $judge_count, $original_prompt)`**
    * **Purpose**: A **helper function for `hill_climbing()`**. It compares two LLM-generated versions (a "best" and a "candidate") based on specified criteria, using multiple LLM calls as "judges" to ensure objective evaluation.
    * **Workflow**:
        1.  Constructs a prompt for an LLM "judge," clearly presenting both the `$best_version` and `$new_candidate`, the `$original_prompt` context, and the `$evaluation_criteria`.
        2.  Executes `call_llm()` `$judge_count` times, each time asking a "judge" to select the better version (indicated by returning `'1'` or `'2'`).
        3.  Tallies the votes from all judges.
    * **Parameters**:
        * `$best_version` (scalar): The content of the currently considered best solution.
        * `$new_candidate` (scalar): The content of the new candidate solution being evaluated.
        * `$evaluation_criteria` (scalar): The string defining the criteria for evaluation.
        * `$judge_count` (integer): The number of LLM "judges" to consult for voting.
        * `$original_prompt` (scalar): The initial generative prompt that produced these versions (provided for judge context).
    * **Returns**: `'1'` if the first version wins the majority vote, `'2'` if the second version wins.

### 1.3 Text Cleaning Function

* **`remove_non_ascii($text)`**
    * **Purpose**: Cleans text by removing or normalizing specific non-ASCII characters that LLMs frequently generate, ensuring output cleanliness and compatibility.
    * **Key Features**:
        * **Removes emoticons**.
        * **Normalizes curly quotes (`“”‘’«»`) and curly apostrophes to straight quotes (`"`, `'`)**.
        * Normalizes em-dashes (`—`) and en-dashes (`–`) to double hyphens (`--`).
        * Normalizes ellipsis (`…`) to three periods (`...`).
        * Removes zero-width characters (e.g., `\x{200B}`).
        * **Crucially**: Preserves accented characters, non-Latin scripts, and common typographical/currency symbols, while removing other general Unicode "control" or "format" characters.
        * Normalizes all line endings to Unix-style (`\n`).
    * **Returns**: The cleaned string.

### 1.4 Tag Extraction Function

* **`extract_text_between_tags($text, $tag, %opts)`**
    * **Purpose**: Extracts content enclosed within specific tags from an LLM response. This function is **designed to be highly robust** against common LLM formatting inconsistencies and missing tags.
    * **Key Features**:
        * **Flexible Tag Matching**: Handles variations in tag syntax (e.g., `< answer >`, `< /answer >`, `<ANSWER>`) and common misspellings (`<answe?r?>`, `<answers?>`).
        * **LLM-Specific Cleanup**: Automatically removes `<model>...</model>` blocks from the response before extraction, as these are often internal LLM thoughts.
        * **Intelligent Boundary Handling**: Can infer a missing closing tag (e.g., `</answer>`) if it's followed by known semantic boundary tags like `<comments>` or `<thinking>`.
        * **Optional Strict Mode**: If `strict => 1` is passed in `%opts`, it requires both a perfectly matched opening and closing tag for extraction; otherwise, it operates in a more flexible mode.
        * **Post-Extraction Cleanup**: Trims leading/trailing whitespace from the extracted content and applies `remove_non_ascii()` to ensure cleanliness.
    * **Parameters**:
        * `$text` (scalar): The full LLM response string.
        * `$tag` (scalar): The name of the tag to extract (e.g., `'answer'`, `'comments'`, `'thinking'`). Case-insensitivity is handled internally.
        * `%opts` (hash, optional): Can include `strict => 1` for strict matching (default is flexible `0`).
    * **Returns**: The extracted and cleaned content as a scalar string, or an empty string (`''`) if content for the specified tag cannot be robustly extracted.

---

## 2. LLM Design Patterns

These are standard design patterns that **must be applied** in Perl LLM projects to ensure structured, predictable, and easily parsable outputs.

### 2.1 Response Tagging (Core Structural Pattern)

* **Intent**: To achieve clear and reliable separation between the LLM's primary answer and any other accompanying content. This is fundamental for automated parsing.
* **Mandatory Tags**:
    * The primary answer **MUST ALWAYS** be enclosed within **`<answer>...</answer>`** tags.
    * Any comments, meta-information, or LLM internal thoughts **MUST ALWAYS** be placed within **`<comments>...</comments>`** tags.
* **When to Use**: **Always**, for any LLM prompt where you need to extract the core response reliably.
* **Function Used**: `extract_text_between_tags()` is specifically designed to work with this pattern.

### 2.2 Comments Section (Core Structural Pattern)

* **Intent**: To explicitly separate explanatory comments, thoughts, or rationale from the main answer, making both the answer and the additional context easily accessible.
* **Implementation**: Achieved by instructing the LLM to use the `<comments>...</comments>` tags as described in the "Response Tagging" pattern.
* **When to Use**: When the LLM might provide useful meta-commentary, its thinking process, or additional context alongside the direct response.

### 2.3 Multiple Response Sections (Core Structural Pattern)

* **Intent**: To capture multiple, distinct structured fields within a single LLM response, allowing for granular data extraction.
* **Mechanism**: Define unique tag pairs for each desired section (e.g., `<title>...</title>`, `<summary>...</summary>`, `<outline>...</outline>`).
* **When to Use**: When a prompt elicits several different, clearly defined pieces of information that need to be extracted individually for downstream processing.
* **Function Used**: `extract_text_between_tags()` can be called multiple times, once for each expected tag, to extract all sections.

### 2.4 Extracting Tagged Content (Core Structural Pattern)

* **Intent**: To enable robust and automated parsing of specific fields from an LLM's structured output. This pattern dictates *how* the data is pulled from the LLM's response.
* **Implementation**: This pattern is **directly implemented** by the `extract_text_between_tags()` function. The LLM **must rely on this function** for all tag-based extraction tasks.
* **When to Use**: For any LLM interaction where the response is structured with custom tags.

---

## 3. Best Practices for LLM Integration

Adhering to these best practices is **non-negotiable** for all LLM development.

### 3.1 Prompt Design Guidelines

* **Structured Output (Mandatory)**:
    * **ALWAYS** instruct the LLM to enclose its core answer within **`<answer>...</answer>`** tags.
    * **ALWAYS** instruct the LLM to place any additional comments or meta-information within **`<comments>...</comments>`** tags.
* **Output Format (Mandatory)**:
    * **DO NOT use JSON for output structure**. While popular, LLMs frequently misformat JSON, leading to parsing errors. Prefer simple, tag-based formats (like the XML-like tags described here) or simple tabular outputs for significantly more consistency and reliability.
    * **Instruct the LLM to use plain, straight quotes (`"` and `'`) and apostrophes only**.
    * **Tell the LLM to avoid emoticons** and other non-standard punctuation or decorative characters unless their inclusion is absolutely and explicitly required for the output.
    * **Tell the LLM to avoid emdashes (`—`) and en-dashes (`–`)**, using double hyphens (`--`) instead.

### 3.2 Output Handling Guidelines

* **Mandatory Tag Extraction**: **ALWAYS** use **`extract_text_between_tags()`** to reliably retrieve structured content from LLM responses. **DO NOT write custom parsing logic for tags.**
* **Mandatory Text Cleaning**: **ALWAYS** apply **`remove_non_ascii()`** to the extracted text for consistent character formatting and removal of unwanted elements.
* **Mandatory Iterative Improvement**: When a task requires reflection, refinement, or a higher quality output through multiple iterations, **ALWAYS** use **`hill_climbing()`**. **Do NOT implement custom iterative loops.**

### 3.3 Standard LLM Integration Workflow

The overall workflow for interacting with LLMs **must** follow these steps:

1.  **Prepare the Prompt**: Construct your prompt with clear and explicit instructions for structured output, **always** specifying the use of `<answer>...</answer>` and `<comments>...</comments>` tags.
2.  **Call the LLM**: Use the **`call_llm()`** function to send the prepared prompt and receive the initial response.
3.  **Extract Structured Output**: Immediately after receiving the response, utilize **`extract_text_between_tags()`** to parse and retrieve the desired content sections (e.g., the main answer, comments, or other custom sections).
4.  **Clean Text**: If any character consistency issues are anticipated or observed, apply **`remove_non_ascii()`** to the extracted text to ensure consistent and clean formatting.
5.  **Iterative Improvement (Conditional)**: If the task requires a refinement loop (e.g., for complex problem-solving, code generation, or creative writing), employ **`hill_climbing()`** to iteratively enhance the quality of the LLM's response.

---

## 4. Agentic Programming Pillars (Conceptual Foundations)

These five foundational principles underpin the development of intelligent, self-improving LLM-based systems. While conceptual, they directly guide the application of the Perl functions and design patterns outlined above.

1.  **Iterative Improvement (Hill Climbing)**:
    * **Concept**: Continuously refine LLM output by generating candidate solutions and using feedback (from a "Second Mind" or a "panel of judges" via `judge_voting()`) to iteratively select and build upon the "best" version until a high-quality result is achieved.
    * **Perl Implementation**: Directly supported by the **`hill_climbing()`** function.
2.  **Atomic Task Breakdown (Granular Task Planning)**:
    * **Concept**: Decompose complex problems into smaller, more manageable, "atomic" subtasks. This simplifies the LLM's workload and makes errors easier to spot and fix.
    * **Application**: Structure your overall Perl script to break down large LLM requests into a series of smaller, sequential `call_llm()` operations, each focusing on a single atomic task.
3.  **The Second Mind (Results and Consistency Checking)**:
    * **Concept**: Implement a dedicated reflective and evaluative layer that reviews, critiques, and corrects the primary LLM's output for completeness, consistency, and alignment with original intent. This prevents "amnesia" where good ideas are lost during refinement.
    * **Perl Implementation**: The critique phase within **`hill_climbing()`** directly embodies this pillar, asking an LLM to act as a "Second Mind" to evaluate and advise on improvements.
4.  **Trusted Transformations (Leveraging LLM Strengths)**:
    * **Concept**: Identify and focus on tasks where LLMs excel (e.g., summarization, translation, code generation, text reformatting, outlining). Delegate these tasks confidently to the LLM.
    * **Application**: Design prompts that specifically request these trusted transformations, rather than relying on the LLM for tasks outside its core competencies.
5.  **Factual Sourcing and Hallucination Mitigation**:
    * **Concept**: Avoid LLM "hallucinations" by **never** relying on them as primary sources for factual data. Instead, supply accurate information from trusted external sources (databases, APIs, user input) directly in your prompts.
    * **Application**: Separate data retrieval (from external sources) from data transformation (by the LLM). Use the LLM to process and format verified data, not to generate facts.

---

## 5. Practical Notes

* **Temporary File Management**: The system **always** uses the `./temp` directory for temporary input and output files for LLM calls.
* **Configuration**: Sensitive data (e.g., API keys) **must** be stored in a separate configuration file (e.g., `openrouter_config.txt`) as referenced by `call_llm()`. **Never hardcode credentials.**
* **Logging**: LLM calls generate logs in the `./logs` directory, which are essential for debugging, monitoring, and analysis.
* **Output Consistency**: Strict adherence to the tag-based output formats (e.g., `<answer>`, `<comments>`) is crucial for the reliability of the `extract_text_between_tags()` function.

---